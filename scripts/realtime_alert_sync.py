from elasticsearch import Elasticsearch
from elasticsearch_dsl import Search
from datetime import datetime, timezone, timedelta
import urllib3
import subprocess
import time
import json
import logging
import os
import hashlib
import pandas as pd
import pytz
from dateutil import parser


from config import (
    REMOTE_USERNAME,
    REMOTE_HOST,
    SSH_TUNNEL_REMOTE_PORT,
    SSH_TUNNEL_LOCAL_PORT,
    ELASTIC_USER,
    ELASTIC_PASS,
    ELASTIC_HOST,
    ALERT_SEVERITY,
    ALERT_OUTPUT_PATH,
)

# --- C·∫§U H√åNH V√Ä C√ÅC H√ÄM C·ªê ƒê·ªäNH ---

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

POLL_INTERVAL = 5
STATE_FILE = "state.json"
LOCAL_TZ = pytz.timezone('Asia/Ho_Chi_Minh')
INDEXING_DELAY_BUFFER = timedelta(seconds=40)
LOOKBACK_WINDOW = timedelta(minutes=3) # <-- TH√äM M·ªöI: ƒê·ªãnh nghƒ©a c·ª≠a s·ªï nh√¨n l·∫°i l√† 3 ph√∫t
UTC = pytz.utc

# (C√°c h√†m kh√¥ng thay ƒë·ªïi gi·ªØ nguy√™n)
def start_ssh_tunnel():
    try:
        local_port = SSH_TUNNEL_LOCAL_PORT
        remote_port = SSH_TUNNEL_REMOTE_PORT
        ssh_target = f"{REMOTE_USERNAME}@{REMOTE_HOST}"
        tunnel_process = subprocess.Popen(['ssh', '-N', '-L', f'{local_port}:localhost:{remote_port}', ssh_target])
        time.sleep(5)
        logging.info("SSH tunnel established successfully.")
        return tunnel_process
    except Exception as e:
        logging.error(f"Error starting SSH tunnel: {e}")
        return None

def connect_elasticsearch():
    try:
        es = Elasticsearch([ELASTIC_HOST], ca_certs=False, verify_certs=False, basic_auth=(ELASTIC_USER, ELASTIC_PASS))
        if es.ping():
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            logging.info("Connected to Elasticsearch.")
            return es
        return None
    except Exception as e:
        logging.error(f"Elasticsearch connection error: {e}")
        return None

def retrieve_alerts(es, severity_list, start_time_str, end_time_str):
    """
    Truy v·∫•n v√† "l√†m ph·∫≥ng" d·ªØ li·ªáu alert ƒë·ªÉ c√°c c·ªôt l·ªìng nhau
    (nh∆∞ source.ip, rule.name) ƒë∆∞·ª£c ƒë∆∞a l√™n c·∫•p cao nh·∫•t.
    """
    try:
        from elasticsearch_dsl import Q

        severity_query = Q('bool',
            should=[
                Q('terms', **{'rule.severity': severity_list}),
                Q('terms', **{'event.severity': severity_list})
            ],
            minimum_should_match=1
        )

        search_context = Search(using=es, index='*logs-*') \
            .query("query_string", query="event.module:suricata") \
            .query(severity_query) \
            .filter('range', **{'@timestamp': {'gte': start_time_str, 'lte': end_time_str}}) \
            .sort('@timestamp')

        alerts_list = [d.to_dict() for d in search_context.scan()]
        if not alerts_list:
            return pd.DataFrame()

        flattened_alerts = []
        for alert in alerts_list:
            flat_alert = {
                '@timestamp': alert.get('@timestamp'),
                'source.ip': alert.get('source', {}).get('ip'),
                'source.port': alert.get('source', {}).get('port'),
                'destination.ip': alert.get('destination', {}).get('ip'),
                'destination.port': alert.get('destination', {}).get('port'),
                'rule.name': alert.get('rule', {}).get('name'),
                'original_alert': alert
            }
            flattened_alerts.append(flat_alert)

        return pd.DataFrame(flattened_alerts)

    except Exception as e:
        logging.error(f"Error retrieving and flattening alerts: {e}")
        return pd.DataFrame()

# --- C√ÅC H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU ƒê∆Ø·ª¢C C·∫¨P NH·∫¨T ---

# T√ÅCH RI√äNG H√ÄM T·∫†O HASH ƒê·ªÇ T√ÅI S·ª¨ D·ª§NG
# def generate_alert_hash(row):
#     """
#     T·∫°o hash cho m·ªôt d√≤ng alert (Series) d·ª±a tr√™n c√°c c·ªôt ƒë·ªãnh danh.
#     """
#     cols_for_uniqueness = [
#         '@timestamp', 'source.ip', 'source.port',
#         'destination.ip', 'destination.port', 'rule.name'
#     ]
#     # Ch·ªâ l·∫•y c√°c c·ªôt t·ªìn t·∫°i trong `row` ƒë·ªÉ tr√°nh l·ªói
#     existing_cols = [col for col in cols_for_uniqueness if col in row.index]
#     unique_tuple = tuple(str(row[col]) for col in existing_cols)
#     return hashlib.sha256(str(unique_tuple).encode('utf-8')).hexdigest()
# FILE: collectors/main_script.py

# T√ÅCH RI√äNG H√ÄM T·∫†O HASH ƒê·ªÇ T√ÅI S·ª¨ D·ª§NG
def generate_alert_hash(row):
    """
    T·∫°o hash cho m·ªôt d√≤ng alert (Series) d·ª±a tr√™n c√°c c·ªôt ƒë·ªãnh danh.
    ƒê√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·ªÉ chu·∫©n h√≥a ki·ªÉu d·ªØ li·ªáu, tr√°nh l·ªói hash kh√¥ng kh·ªõp.
    """
    cols_for_uniqueness = [
        '@timestamp', 'source.ip', 'source.port',
        'destination.ip', 'destination.port', 'rule.name'
    ]
    existing_cols = [col for col in cols_for_uniqueness if col in row.index]
    
    values = []
    for col in existing_cols:
        val = row[col]
        # B·ªè qua c√°c gi√° tr·ªã r·ªóng ƒë·ªÉ tr√°nh l·ªói
        if val is None or pd.isna(val):
            values.append('') # D√πng chu·ªói r·ªóng thay th·∫ø
            continue

        # --- ƒê√ÇY L√Ä PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG ---
        # Chu·∫©n h√≥a c√°c tr∆∞·ªùng port v·ªÅ ki·ªÉu s·ªë nguy√™n tr∆∞·ªõc khi chuy·ªÉn th√†nh chu·ªói.
        # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o str(54066) v√† str(54066.0) ƒë·ªÅu cho ra k·∫øt qu·∫£ nh·∫•t qu√°n.
        if col.endswith('.port'):
            try:
                # D√πng float() tr∆∞·ªõc ƒë·ªÉ x·ª≠ l√Ω c·∫£ '54066' v√† '54066.0'
                values.append(str(int(float(val))))
            except (ValueError, TypeError):
                values.append(str(val)) # N·∫øu kh√¥ng ph·∫£i s·ªë th√¨ gi·ªØ nguy√™n
        else:
            values.append(str(val))
            
    unique_tuple = tuple(values)
    return hashlib.sha256(str(unique_tuple).encode('utf-8')).hexdigest()

def deduplicate_alerts(df):
    """
    Lo·∫°i b·ªè c√°c alert tr√πng l·∫∑p trong m·ªôt DataFrame (m·ªôt m·∫ª l·∫•y v·ªÅ).
    """
    if df.empty:
        return df
    
    df['alert_hash'] = df.apply(generate_alert_hash, axis=1)
    deduplicated_df = df.drop_duplicates(subset='alert_hash', keep='first')
    return deduplicated_df.drop(columns=['alert_hash'])

# H√ÄM M·ªöI: L·ªåC C√ÅC ALERT ƒê√É T·ªíN T·∫†I TRONG FILE LOG
def filter_against_log(new_alerts_df, log_file_path):
    """
    L·ªçc DataFrame alert m·ªõi, lo·∫°i b·ªè nh·ªØng alert ƒë√£ t·ªìn t·∫°i trong file log.
    """
    if new_alerts_df.empty:
        return new_alerts_df
        
    try:
        if not os.path.exists(log_file_path) or os.path.getsize(log_file_path) == 0:
            # N·∫øu file log kh√¥ng t·ªìn t·∫°i ho·∫∑c r·ªóng, kh√¥ng c·∫ßn l·ªçc
            return new_alerts_df

        # ƒê·ªçc file log hi·ªán t·∫°i
        existing_alerts_df = pd.read_json(log_file_path, lines=True)
        if existing_alerts_df.empty:
            return new_alerts_df

        # T·∫°o hash cho c√°c alert ƒë√£ c√≥ trong log
        existing_hashes = set(existing_alerts_df.apply(generate_alert_hash, axis=1))
        
        # T·∫°o hash cho c√°c alert m·ªõi
        new_alerts_df['alert_hash'] = new_alerts_df.apply(generate_alert_hash, axis=1)

        # L·ªçc ra nh·ªØng alert c√≥ hash CH∆ØA xu·∫•t hi·ªán trong log
        truly_new_alerts_df = new_alerts_df[~new_alerts_df['alert_hash'].isin(existing_hashes)]

        return truly_new_alerts_df.drop(columns=['alert_hash'])

    except Exception as e:
        logging.error(f"Error filtering against log file {log_file_path}: {e}")
        # Trong tr∆∞·ªùng h·ª£p l·ªói, tr·∫£ v·ªÅ DataFrame g·ªëc ƒë·ªÉ tr√°nh m·∫•t d·ªØ li·ªáu
        return new_alerts_df

# ----- C√ÅC H√ÄM X·ª¨ L√ù TH·ªúI GIAN ƒê∆Ø·ª¢C CHU·∫®N H√ìA V·ªÄ UTC -----

def save_last_timestamp(ts_obj):
    try:
        ts_utc_obj = ts_obj.astimezone(UTC)
        with open(STATE_FILE, 'w') as f:
            json.dump({'last_timestamp': ts_utc_obj.isoformat()}, f)
    except Exception as e:
        logging.error(f"Could not save state to {STATE_FILE}: {e}")

def load_last_timestamp():
    try:
        with open(STATE_FILE, 'r') as f:
            data = json.load(f)
            last_ts_str = data.get('last_timestamp')
            if last_ts_str:
                ts_obj = parser.parse(last_ts_str)
                ts_utc_obj = ts_obj.astimezone(UTC)
                logging.info(f"Loaded state. Resuming from {ts_utc_obj.isoformat()}")
                return ts_utc_obj
    except Exception:
        logging.warning(f"State file not found or invalid. Starting from a recent past time.")
    
    return (datetime.now(LOCAL_TZ) - INDEXING_DELAY_BUFFER).astimezone(UTC)

# --- KH·ªêI TH·ª∞C THI CH√çNH - PHI√äN B·∫¢N C·∫¨P NH·∫¨T ---
if __name__ == "__main__":
    
    try:
        if os.path.exists(STATE_FILE):
            os.remove(STATE_FILE)
            logging.info(f"Removed old state file: {STATE_FILE}")
    except Exception as e:
        logging.error(f"Error removing state file {STATE_FILE}: {e}")
        
    tunnel_proc = start_ssh_tunnel()
    if not tunnel_proc: exit()

    es = connect_elasticsearch()
    if not es:
        tunnel_proc.terminate()
        exit()

    last_ts_utc_obj = load_last_timestamp()

    logging.info(f"Starting real-time alert monitoring. All times are handled in UTC.")
    logging.info(f"Press Ctrl+C to stop.")

    try:
        while True:
            # --- X√°c ƒë·ªãnh c√°c kho·∫£ng th·ªùi gian ---
            # 1. Kho·∫£ng th·ªùi gian ch√≠nh (ti·∫øn t·ªõi)
            start_time_main_utc_obj = last_ts_utc_obj
            end_time_utc_obj = (datetime.now(LOCAL_TZ) - INDEXING_DELAY_BUFFER).astimezone(UTC)

            # 2. Kho·∫£ng th·ªùi gian nh√¨n l·∫°i (look-back)
            start_time_lookback_utc_obj = end_time_utc_obj - LOOKBACK_WINDOW
            
            if start_time_main_utc_obj >= end_time_utc_obj:
                logging.warning(f"Time window invalid (start >= end): {start_time_main_utc_obj.isoformat()} >= {end_time_utc_obj.isoformat()}. Skipping.")
                time.sleep(POLL_INTERVAL)
                continue

            # --- Th·ª±c hi·ªán 2 truy v·∫•n ---
            # Truy v·∫•n ch√≠nh
            start_time_main_str = start_time_main_utc_obj.isoformat()
            end_time_main_str = end_time_utc_obj.isoformat()
            logging.info(f"ü™ü  CHECKIN MAIN window from {start_time_main_str} to {end_time_main_str}")
            main_alerts_df = retrieve_alerts(es, ALERT_SEVERITY, start_time_main_str, end_time_main_str)
            
            # Truy v·∫•n look-back
            start_time_lookback_str = start_time_lookback_utc_obj.isoformat()
            end_time_lookback_str = end_time_utc_obj.isoformat()
            logging.info(f"üìö CHECKING LOOKBACK window from {start_time_lookback_str} to {end_time_lookback_str}")
            lookback_alerts_df = retrieve_alerts(es, ALERT_SEVERITY, start_time_lookback_str, end_time_lookback_str)
            
            # --- G·ªôp v√† x·ª≠ l√Ω d·ªØ li·ªáu ---
            combined_alerts_df = pd.concat([main_alerts_df, lookback_alerts_df], ignore_index=True)
            
            # Kh·ª≠ tr√πng l·∫∑p trong m·∫ª d·ªØ li·ªáu v·ª´a l·∫•y v·ªÅ
            unique_batch_df = deduplicate_alerts(combined_alerts_df)
            
            # X√°c ƒë·ªãnh file log c·ªßa ng√†y h√¥m nay
            today_str = datetime.now(LOCAL_TZ).strftime('%Y-%m-%d')
            output_dir = os.path.dirname(ALERT_OUTPUT_PATH)
            base_filename = os.path.basename(ALERT_OUTPUT_PATH).rsplit('.', 1)[0]
            daily_log_path = os.path.join(output_dir, f"{base_filename}-{today_str}.jsonl")
            if not os.path.exists(output_dir): os.makedirs(output_dir)
            if not os.path.exists(daily_log_path):
                open(daily_log_path, 'a').close()
                logging.info(f"Created empty log file for today: {daily_log_path}")

            # L·ªçc l·∫°i v·ªõi c√°c alert ƒë√£ c√≥ trong file log ƒë·ªÉ ch·ªâ l·∫•y ra nh·ªØng alert th·ª±c s·ª± m·ªõi
            truly_new_alerts_df = filter_against_log(unique_batch_df, daily_log_path)
            
            if not truly_new_alerts_df.empty:
                truly_new_alerts_df = truly_new_alerts_df.sort_values(by='@timestamp').reset_index(drop=True)
                logging.info(f"--------------------------------------------------------------------")
                logging.info(f" ‚úÖ Found {len(truly_new_alerts_df)} TRULY new alerts. Appending to log.")
                logging.info(f"--------------------------------------------------------------------")
                jsonl_output = truly_new_alerts_df.to_json(orient='records', lines=True, force_ascii=False)
                with open(daily_log_path, 'a', encoding='utf-8') as f:
                    # Ghi t·ª´ng d√≤ng ƒë·ªÉ tr√°nh l·ªói n·∫øu c√≥ alert kh√¥ng h·ª£p l·ªá
                    for line in jsonl_output.strip().split('\n'):
                        f.write(line + '\n')

                # C·∫≠p nh·∫≠t last_timestamp d·ª±a tr√™n alert m·ªõi nh·∫•t ƒë√£ ƒë∆∞·ª£c ghi
                last_alert_ts_str = truly_new_alerts_df.iloc[-1]['@timestamp']
                last_ts_utc_obj = parser.parse(last_alert_ts_str).astimezone(UTC) + timedelta(milliseconds=1)
            else:
                logging.info("‚ùå No new alerts found in this interval.")
                # N·∫øu kh√¥ng c√≥ alert m·ªõi, last_ts v·∫´n ti·∫øn t·ªõi end_time c·ªßa chu k·ª≥ n√†y
                last_ts_utc_obj = end_time_utc_obj

            save_last_timestamp(last_ts_utc_obj)
            time.sleep(POLL_INTERVAL)

    except KeyboardInterrupt:
        logging.info("Script stopped by user (Ctrl+C).")
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}", exc_info=True)
    finally:
        logging.info("Shutting down.")
        if tunnel_proc:
            tunnel_proc.terminate()