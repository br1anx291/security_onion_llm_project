import pandas as pd
import json
import glob
import os
from tqdm import tqdm
import logging

# --- C·∫§U H√åNH B∆Ø·ªöC 1 ---
CSV_FILES_PATTERN = '*_ISCX.csv'
ALERTS_FILE = 'alerts_CICIDS2017.json'
OUTPUT_MAPPED_FILE = 'mapped_alerts.csv'

PROTOCOL_MAP = {'TCP': 6, 'UDP': 17, 'ICMP': 1}
# --- K·∫æT TH√öC C·∫§U H√åNH ---

def create_connection_lookup_table(csv_pattern):
    """
    ƒê·ªçc t·∫•t c·∫£ c√°c file CSV, gom nh√≥m theo 5-tuple v√† t·ªïng h·ª£p T·∫§T C·∫¢ c√°c nh√£n duy nh·∫•t.
    """
    logging.info(f"ƒêang ƒë·ªçc v√† g·ªôp c√°c file CSV t·ª´ m·∫´u: '{csv_pattern}'...")
    csv_files = glob.glob(csv_pattern)
    if not csv_files:
        logging.error("Kh√¥ng t√¨m th·∫•y file CSV n√†o. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return None

    required_cols = ['Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Protocol', 'Label']
    list_of_dfs = []
    for f in tqdm(csv_files, desc="   -> ƒêang t·∫£i file CSV"):
        try:
            df = pd.read_csv(f, usecols=lambda c: c.strip() in required_cols, encoding='latin-1', low_memory=False)
            df.columns = df.columns.str.strip()
            list_of_dfs.append(df)
        except Exception as e:
            logging.warning(f"L·ªói khi ƒë·ªçc file {f}: {e}")

    if not list_of_dfs:
        logging.error("Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c file CSV n√†o. D·ª´ng ch∆∞∆°ng tr√¨nh.")
        return None

    logging.info("ƒêang g·ªôp v√† x·ª≠ l√Ω 'B·∫£ng tra c·ª©u'...")
    connections_df = pd.concat(list_of_dfs, ignore_index=True)
    
    connections_df.rename(columns={
        'Source IP': 'source_ip', 'Source Port': 'source_port',
        'Destination IP': 'destination_ip', 'Destination Port': 'destination_port',
        'Protocol': 'protocol', 'Label': 'original_label'
    }, inplace=True)

    # N√ÇNG C·∫§P: Gom nh√≥m theo 5-tuple v√† t·ªïng h·ª£p c√°c nh√£n duy nh·∫•t
    logging.info("Gom nh√≥m theo 5-tuple v√† t·ªïng h·ª£p c√°c nh√£n duy nh·∫•t...")
    # B·ªè qua c√°c d√≤ng c√≥ gi√° tr·ªã null trong c√°c c·ªôt ch√≠nh
    connections_df.dropna(subset=['source_ip', 'source_port', 'destination_ip', 'destination_port', 'protocol'], inplace=True)
    
    # √âp ki·ªÉu d·ªØ li·ªáu ƒë·ªÉ groupby ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh
    for col in ['source_port', 'destination_port', 'protocol']:
        connections_df[col] = connections_df[col].astype(int)
        
    lookup_table = connections_df.groupby(
        ['source_ip', 'source_port', 'destination_ip', 'destination_port', 'protocol']
    )['original_label'].unique().apply(list)
    
    logging.info(f"ƒê√£ t·∫°o xong 'B·∫£ng tra c·ª©u' v·ªõi {len(lookup_table)} connections duy nh·∫•t.")
    return lookup_table

def main_step1():
    print("üöÄ B∆Ø·ªöC 1: B·∫Øt ƒë·∫ßu t·∫°o 'Si√™u B·∫£ng' (mapped_alerts.csv)...")
    
    connections_lookup = create_connection_lookup_table(CSV_FILES_PATTERN)
    if connections_lookup is None:
        return

    try:
        with open(ALERTS_FILE, 'r', encoding='utf-8') as f:
            all_alerts = json.load(f)
    except Exception as e:
        print(f"‚ùå L·ªói: Kh√¥ng th·ªÉ ƒë·ªçc file alert '{ALERTS_FILE}'. L√Ω do: {e}")
        return

    print(f"üîé ƒêang l·∫∑p qua {len(all_alerts)} alert ƒë·ªÉ t√¨m nh√£n g·ªëc...")
    mapped_alerts_data = []
    for alert in tqdm(all_alerts, desc="   -> ƒêang √°nh x·∫° alert"):
        src_ip = alert.get('source', {}).get('ip')
        src_port = alert.get('source', {}).get('port')
        dest_ip = alert.get('destination', {}).get('ip')
        dest_port = alert.get('destination', {}).get('port')
        transport_str = alert.get('network', {}).get('transport')
        protocol = PROTOCOL_MAP.get(str(transport_str).upper(), -1)

        if not all([src_ip, src_port, dest_ip, dest_port, protocol != -1]):
            continue

        try:
            # Tra c·ª©u trong b·∫£ng, k·∫øt qu·∫£ tr·∫£ v·ªÅ l√† m·ªôt list, v√≠ d·ª•: ['BENIGN'] ho·∫∑c ['BENIGN', 'DDoS']
            match_labels = connections_lookup.loc[(src_ip, src_port, dest_ip, dest_port, protocol)]
            
            # N√ÇNG C·∫§P: Chuy·ªÉn list c√°c label th√†nh m·ªôt chu·ªói duy nh·∫•t, s·∫Øp x·∫øp ƒë·ªÉ nh·∫•t qu√°n
            original_label_str = ', '.join(map(str, sorted(match_labels)))
            
            mapped_alerts_data.append({
                'timestamp': alert.get('@timestamp'),
                'alert_name': alert.get('rule', {}).get('name'),
                'category': alert.get('rule', {}).get('category'),
                'severity': alert.get('rule', {}).get('severity'),
                'source_ip': src_ip,
                'source_port': src_port,
                'destination_ip': dest_ip,
                'destination_port': dest_port,
                'transport': transport_str,
                'original_label': original_label_str # Ghi chu·ªói ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
            })
        except KeyError:
            continue

    if not mapped_alerts_data:
        print("‚ö†Ô∏è Kh√¥ng c√≥ alert n√†o ƒë∆∞·ª£c √°nh x·∫°. Vui l√≤ng ki·ªÉm tra l·∫°i d·ªØ li·ªáu.")
        return

    print(f"\nüéâ Ho√†n th√†nh B∆∞·ªõc 1! ƒê√£ √°nh x·∫° th√†nh c√¥ng {len(mapped_alerts_data)} alert.")
    print(f"   -> ƒêang l∆∞u v√†o file '{OUTPUT_MAPPED_FILE}'...")
    
    df_mapped = pd.DataFrame(mapped_alerts_data)
    df_mapped.to_csv(OUTPUT_MAPPED_FILE, index=False, encoding='utf-8')
    print("   -> ƒê√£ l∆∞u file th√†nh c√¥ng!")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
    main_step1()